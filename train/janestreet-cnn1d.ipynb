{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ../input/torchlrfinder/torch_lr_finder-0.2.1-py3-none-any.whl\n!pip install ../input/torchsummary/torchsummary-1.5.1-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### notes\n\n- v2: Reduces the number of channels to 32 & 64, adds an intermediate output layer.\n- v3: Reduces `weight_decay` to `1e-4`.\n- v4: IterativeImputer set to 1 iteration & 50 features.\n- v5: IterativeImputer set to 1 iteration & 30 features. `CNN1D(*, sign_size=22, cha_1=16, cha_2=32, cha_3=32, dropout_in1=0.2, dropout_in2=0.1, dropout_mid=0.3, dropout_out=0.2)`.\n- v6: Hyperparams: `{'sign_size': 22, 'cha_1': 16, 'cha_2': 32, 'cha_3': 32, 'input_dim': 131, 'output_dim': 5, 'dropout_in1': 0.25, 'dropout_in2': 0.4, 'dropout_mid': 0.4, 'dropout_out': 0.25, 'weight_decay': 0.0001, 'pct_start': 0.1, 'max_lr': 0.07}`.\n- v7: Keeps state of optimizer, hyperparams: `sign_size=16, cha_1=32, cha_2=64, cha_3=64, dropout_in1=0.25, dropout_in2=0.3, dropout_mid=0.4, dropout_out=0.2`.\n- v8: Removed intermediate output layer, input activation set to `celu`, hyperparams: `sign_size=8, cha_1=32, cha_2=64, cha_3=64, dropout_in1=0.25, dropout_in2=0.3, dropout_mid=0.3, dropout_out=0.15)`.\n- v9: Add NormalLinear layer\n- v10: Major refactoring of model structure (depthwise convolution, higher regularization, lower lr, swa).\n- v11: Hyperparams: `sign_size=16, cha_input=32, cha_hidden=48, K=2, dropout_input=0.2, dropout_hidden=0.25, dropout_output=0.25, weight_decay=1e-4, max_lr=1e-2, pct_start=0.1`.\n- v11: Trained with lower regularization dropout_input=0.2, dropout_hidden=0.25, dropout_output=0.15 and momentum=0.8 on SWA.\n- v12: Changed hyperparams to `sign_size=8, cha_input=32, cha_hidden=64, K=2, dropout_input=0.20, dropout_hidden=0.25, dropout_output=0.15, weight_decay=1e-3`.\n- v13: Changed hyperparams `dropout_input=0.25, dropout_hidden=0.3, dropout_output=0.15`. SWA has st (15 epochs) and lt (30 epochs) version."},{"metadata":{"trusted":true},"cell_type":"code","source":"import copy\nimport numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom category_encoders.one_hot import OneHotEncoder\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset,TensorDataset,DataLoader\nfrom torch_lr_finder import LRFinder\nfrom torchsummary import summary\n\n# custom modules\nimport sys\nsys.path.append(\"../usr/lib/janestreet_torch_utils\")\nfrom janestreet_torch_utils import Monitor, train_step, valid_step\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    \nset_seed(2)\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score(date, weight, resp, action):\n    \"\"\"\n    Fast computation of utility score\n    \"\"\"\n    date = date.astype(int)\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return -u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_encoder(X):\n    \"\"\"\n    Fast one-hot encoding of feature_0\n    \"\"\"\n    X[\"feature_00\"] = 0\n    idx00 = X.query(\"feature_0 == -1\").index\n    X.loc[idx00,\"feature_00\"] = 1\n    \n    X[\"feature_01\"] = 0\n    idx01 = X.query(\"feature_0 == 1\").index\n    X.loc[idx01,\"feature_01\"] = 1\n    \n    return X.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_metrics(monitor):\n    x = np.arange(len(monitor.train_loss))\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(21, 7))\n    \n    ax1 = axes[0]\n    ax2 = ax1.twinx()\n    ax1.plot(x, monitor.train_loss, 'go-', label=\"train_loss\")\n    ax2.plot(x, monitor.train_metric, 'ro-', label=\"train_metric\")\n    plt.legend(loc=\"best\")\n    ax1.set_xlabel('epochs')\n    ax1.set_ylabel('loss')\n    ax1.set_title(\"Training\")\n    plt.grid()\n    \n    ax1 = axes[1]\n    ax2 = ax1.twinx()\n    ax1.plot(x, monitor.valid_loss, 'go-', label=\"valid_loss\")\n    ax2.plot(x, monitor.valid_metric, 'ro-', label=\"valid_metric\")\n    plt.legend(loc=\"best\")\n    ax1.set_xlabel('epochs')\n    ax2.set_ylabel('metric')\n    ax1.set_title(\"Validation\")\n    plt.grid()\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"root = Path(\"../input/janestreet-preprocessing\")\n\ntrain = pd.read_parquet(root/\"train.parquet\")\nfeatures = pd.read_parquet(root/\"features.parquet\")\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query(\"date > 85\").query(\"weight > 0\").reset_index(drop=True)\n\ninput_features = [col for col in train.columns if \"feature\" in col]\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nw_cols = [\"w\", \"w1\", \"w2\", \"w3\", \"w4\"]\n\nX_dset = train.loc[:,input_features].copy()\ny_dset = (train.loc[:,resp_cols] > 0).astype(int).copy()\nw_dset = train.loc[:, w_cols].copy()\ndwr_dset = train.loc[:, [\"date\",\"weight\",\"resp\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nwith open(\"../input/janestreet-imputer/imputer_f0m1.pickle\", \"rb\") as file:\n    imputer_f0m1 = pickle.load(file)\n    file.close()\n    \nwith open(\"../input/janestreet-imputer/imputer_f0p1.pickle\", \"rb\") as file:\n    imputer_f0p1 = pickle.load(file)\n    file.close()\n    \n    \nidx_f0m1 = X_dset.query(\"feature_0 == -1\").index\nX_dset.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_dset.loc[idx_f0m1, input_features[1:]])\n\nidx_f0p1 = X_dset.query(\"feature_0 ==  1\").index\nX_dset.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_dset.loc[idx_f0p1, input_features[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dset = cat_encoder(X_dset)\ninput_features = X_dset.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"class NormalLinear(nn.Module):\n    \"\"\" \n    Linear layer with normalized weights\n    \"\"\"\n    def __init__(self, size_in, size_out, bias=True):\n        super(NormalLinear, self).__init__()\n        self.size_in, self.size_out = size_in, size_out\n        # weights vector\n        weights_v = torch.Tensor(size_out, size_in)\n        nn.init.kaiming_uniform_(weights_v, a=np.sqrt(5)) \n        self.weights_v = nn.Parameter(weights_v)\n        # weights magnitude\n        weights_m = torch.norm(weights_v, dim=1, keepdim=True)\n        self.weights_m = nn.Parameter(weights_m.clone().detach())\n        \n        if bias:\n            bias_v = torch.Tensor(size_out)    \n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weights_v)\n            bound = 1 / np.sqrt(fan_in)\n            nn.init.uniform_(bias_v, -bound, bound)\n            self.bias = nn.Parameter(bias_v)\n        else:\n            self.register_parameter('bias', None)\n            \n    def _compute_weights(self):\n        norm_per_output = torch.norm(self.weights_v, dim=1, keepdim=True)\n        return self.weights_m * torch.div(self.weights_v, norm_per_output)\n            \n    def forward(self, x):\n        weights = self._compute_weights()\n        return nn.functional.linear(x, weights, self.bias)\n\n\nclass CNN1D(nn.Module):\n    def __init__(self, input_dim, output_dim, sign_size=16, cha_input=32, cha_hidden=32, K=2,\n                 dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):\n        super(CNN1D, self).__init__()\n\n        hidden_size = sign_size*cha_input\n        sign_size1 = sign_size\n        sign_size2 = sign_size//2\n        output_size = (sign_size//4) * cha_hidden\n\n        self.hidden_size = hidden_size\n        self.cha_input = cha_input\n        self.cha_hidden = cha_hidden\n        self.K = K\n        self.sign_size1 = sign_size1\n        self.sign_size2 = sign_size2\n        self.output_size = output_size\n        self.dropout_input = dropout_input\n        self.dropout_hidden = dropout_hidden\n        self.dropout_output = dropout_output\n\n        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n        self.dropout1 = nn.Dropout(dropout_input)\n        self.dense1 = NormalLinear(input_dim, hidden_size, bias=False)\n\n        # 1st conv layer\n        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n        self.conv1 = nn.Conv1d(cha_input, cha_input*K, kernel_size = 5, stride = 1, padding=2,  groups=cha_input, bias=False)\n\n        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = sign_size2)\n\n        # 2nd conv layer\n        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)\n        self.dropout_c2 = nn.Dropout(dropout_hidden)\n        self.conv2 = nn.Conv1d(cha_input*K, cha_hidden, kernel_size = 3, stride = 1, padding=1, bias=False)\n\n        # 3rd conv layer\n        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n        self.dropout_c3 = nn.Dropout(dropout_hidden)\n        self.conv3 = nn.Conv1d(cha_hidden, cha_hidden, kernel_size = 3, stride = 1, padding=1, bias=False)\n\n        # 4th conv layer\n        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n        self.conv4 = nn.Conv1d(cha_hidden, cha_hidden, kernel_size = 5, stride = 1, padding=2, groups=cha_hidden, bias=False)\n\n        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        self.batch_norm2 = nn.BatchNorm1d(output_size)\n        self.dropout2 = nn.Dropout(dropout_output)\n        self.dense2 = NormalLinear(output_size, output_dim, bias=False)\n\n    def forward(self, x):\n\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.celu(self.dense1(x))\n\n        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n\n        x = self.batch_norm_c1(x)\n        x = F.relu(self.conv1(x))\n\n        x = self.ave_po_c1(x)\n\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = F.relu(self.conv2(x))\n        x_s = x\n\n        x = self.batch_norm_c3(x)\n        x = self.dropout_c3(x)\n        x = F.relu(self.conv3(x))\n\n        x = self.batch_norm_c4(x)\n        x = self.conv4(x)\n        x =  x + x_s\n        x = F.relu(x)\n\n        x = self.avg_po_c4(x)\n\n        x = self.flt(x)\n\n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = self.dense2(x)\n\n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BCELabelSmoothing(nn.Module):\n    def __init__(self, label_smoothing=0.0):\n        super(BCELabelSmoothing, self).__init__()\n        self.label_smoothing = label_smoothing\n        self.bce_loss = torch.nn.functional.binary_cross_entropy_with_logits\n        \n    def forward(self, prediction, target, weight=None):\n        target_smooth = target*(1.0 - self.label_smoothing) + 0.5*self.label_smoothing\n        if weight is None:\n            loss = self.bce_loss(prediction, target_smooth, reduction=\"mean\")\n        else:\n            loss = self.bce_loss(prediction, target_smooth, weight, reduction=\"sum\") / torch.sum(weight)\n        return loss\n\nbce_loss = BCELabelSmoothing(label_smoothing=1e-2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### model training: 1st step"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = train.query(\"date < 450\").index\nvalid_idx = train.query(\"date >= 450\").index\n\ntrain_dset = TensorDataset(torch.tensor(X_dset.loc[train_idx].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[train_idx].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[train_idx].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[train_idx].values, dtype=torch.float),\n                          )\n\nvalid_dset = TensorDataset(torch.tensor(X_dset.loc[valid_idx].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[valid_idx].values, dtype=torch.float),\n                          )\n\ndataset_sizes = {'train': len(train_dset), 'valid': len(valid_dset)}\ntrain_dataloader = DataLoader(train_dset, batch_size=2048, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nvalid_dataloader = DataLoader(valid_dset, batch_size=len(valid_dset), shuffle=False, num_workers=2, pin_memory=True, drop_last=True)\n\nprint(\"Number of step per epoch:\", len(train_dset)//2048)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_model = CNN1D(input_dim=len(input_features), output_dim=len(resp_cols),\n               sign_size=8, cha_input=32, cha_hidden=64, K=2,\n               dropout_input=0.25, dropout_hidden=0.3, dropout_output=0.15)\n_model = _model.to(device)\n\n_optimizer = torch.optim.SGD(_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-3)\nlr_finder = LRFinder(_model, _optimizer, bce_loss, device=\"cuda\")\nlr_finder.range_test(train_dataloader, start_lr=1e-4, end_lr=1e1, num_iter=652*2, step_mode=\"exp\")\nlr_finder.plot(show_lr=1e-2)\nplt.show()\n\n_optimizer = torch.optim.SGD(_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-3)\nlr_finder = LRFinder(_model, _optimizer, bce_loss, device=\"cuda\")\nlr_finder.range_test(train_dataloader, start_lr=1e-4, end_lr=1e1, num_iter=652*2, step_mode=\"exp\")\nlr_finder.plot(show_lr=1e-2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = CNN1D(input_dim=len(input_features), output_dim=len(resp_cols),\n              sign_size=8, cha_input=32, cha_hidden=64, K=2,\n              dropout_input=0.25, dropout_hidden=0.3, dropout_output=0.15)\nmodel = model.to(device)\nsummary(model, input_size=(len(input_features),))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_history = list()\n\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-3)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, \n    max_lr=1e-2,\n    epochs=50,\n    pct_start=0.3, \n    anneal_strategy='cos', \n    cycle_momentum=True, \n    base_momentum=0.8, \n    max_momentum=0.9, \n    div_factor=1e1,\n    final_div_factor=1e0,\n    steps_per_epoch=len(train_dataloader),\n    verbose=False)\n\nmonitor = Monitor(\n    model=model,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    patience=10,\n    metric_fn=utility_score,\n    experiment_name=f'cnn1d',\n    num_epochs=50,\n    dataset_sizes=dataset_sizes,\n    early_stop_on_metric=False,\n    lower_is_better=True)\n\nfor epoch in monitor.iter_epochs:\n    train_step(model, train_dataloader, optimizer, monitor, bce_loss, scheduler=scheduler, clip_value=None)    \n    early_stop = valid_step(model, valid_dataloader, optimizer, monitor, bce_loss)\n    models_history.append(copy.deepcopy(model))\n    if early_stop: break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics(monitor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saves all the models\nfor epoch,_model in enumerate(models_history):\n    torch.save(_model.state_dict(), f\"./cnn1d-step1-epoch{epoch+1}.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWA around optimal on validation data\nswa_model = torch.optim.swa_utils.AveragedModel(model)\nbest_epoch = len(models_history)-11\n\nfor _model in models_history[best_epoch:]:\n    swa_model.update_parameters(_model)\n    \ntorch.optim.swa_utils.update_bn(train_dataloader, swa_model, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# puts state_dict of SWA in the format of original model\nstate_dict = OrderedDict()\nstate_dict_swa = swa_model.state_dict()\n\nfor key,params in state_dict_swa.items():\n    if key == \"n_averaged\": continue\n    key = key.replace(\"module.\",\"\")\n    state_dict[key] = params\n    \n# restore model to best averaged state\nmodel.load_state_dict(state_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### model training: 2nd step"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = train.query(\"date < 450\").index\nvalid_idx = train.query(\"date >= 450\").index\n\ntrain_dset = TensorDataset(torch.tensor(X_dset.loc[:].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[:].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[:].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[:].values, dtype=torch.float),\n                          )\n\nvalid_dset = TensorDataset(torch.tensor(X_dset.loc[valid_idx].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[valid_idx].values, dtype=torch.float),\n                          )\n\ndataset_sizes = {'train': len(train_dset), 'valid': len(valid_dset)}\ntrain_dataloader = DataLoader(train_dset, batch_size=2048, shuffle=True, num_workers=2)\nvalid_dataloader = DataLoader(valid_dset, batch_size=len(valid_dset), shuffle=False, num_workers=2)\n\nprint(\"Number of step per epoch:\", len(train_dset)//2048)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.8, weight_decay=1e-3)\nlr_finder = LRFinder(model, _optimizer, bce_loss, device=\"cuda\")\nlr_finder.range_test(train_dataloader, start_lr=1e-4, end_lr=1e1, num_iter=652*4, step_mode=\"exp\")\nlr_finder.plot(show_lr=1e-2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# restore to state of best epoch\nmodel.load_state_dict(state_dict)\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.8, weight_decay=1e-3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=5e-3, anneal_strategy=\"cos\", anneal_epochs=5)\n\nmodels_history = list()\n\nmonitor = Monitor(\n    model=model,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    patience=35,\n    metric_fn=utility_score,\n    experiment_name=f'cnn1d',\n    num_epochs=35,\n    dataset_sizes=dataset_sizes,\n    early_stop_on_metric=False,\n    lower_is_better=True)\n\nfor epoch in monitor.iter_epochs:\n    train_step(model, train_dataloader, optimizer, monitor, bce_loss, scheduler=None, clip_value=None)    \n    valid_step(model, valid_dataloader, optimizer, monitor, bce_loss)\n    scheduler.step()\n    \n    models_history.append(copy.deepcopy(model))\n    \n    # save models along the path of swa\n    torch.save(model.state_dict(), f\"./cnn1d-epoch{epoch+1}.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics(monitor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWA short training\nstart_epoch = 5\nend_epoch = 20\n\nswa_model_st = torch.optim.swa_utils.AveragedModel(model)\nswa_model_st = swa_model_st.to(device)\n\nfor _model in models_history[start_epoch:end_epoch]:\n    swa_model_st.update_parameters(_model)\n    \ntorch.optim.swa_utils.update_bn(train_dataloader, swa_model_st, device=device)\n\n# save final model for inference\ntorch.save(swa_model_st.state_dict(), \"./cnn1d-swa-st.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWA long training\nstart_epoch = 5\nend_epoch = 35\n\nswa_model_lt = torch.optim.swa_utils.AveragedModel(model)\nswa_model_lt = swa_model_lt.to(device)\n\nfor _model in models_history[start_epoch:end_epoch]:\n    swa_model_lt.update_parameters(_model)\n    \ntorch.optim.swa_utils.update_bn(train_dataloader, swa_model_lt, device=device)\n\n# save final model for inference\ntorch.save(swa_model_lt.state_dict(), \"./cnn1d-swa-lt.pt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}