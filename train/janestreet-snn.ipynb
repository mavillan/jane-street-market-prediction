{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/torchlrfinder/torch_lr_finder-0.2.1-py3-none-any.whl\n!pip install ../input/torchsummary/torchsummary-1.5.1-py3-none-any.whl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### Notes\n\n- v9: Changed IterativeImputer to SimpleImputer\n- v10: Back to IterativeImputer, standard BatchNorm1d in input layer, increased dropout rate of input layer (x2), reduced lr (1e-4) of second stage model, increased factor to 0.5 ReduceLROnPlateau.\n- v11: Dropout of input layer (x1), lr of first step 1e-4, lr of second step 1e-4, patience set to 4, weight_decay set to 1e-4.\n- v12: Changed optimizers to `torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9, weight_decay=1e-5)`\n- v13: Removed BN from input layer, added RobustScaler on full dataset, changed `batch_size=2048` and `virtual_batch_size=256`, reduced `lr=5e-2` and `lr=1e-2` respectively, increased `label_smoothing=1e-2` and `early_stop_on_metric` for the second stage model.\n- v14: Removed RobustScaler, added GBN at input layer, set `virtual_batch_size=512`, set `weight_decay=1e-4`, set `patience=20` (early stopping) on second model.\n- v15: Changed ReduceLROnPlateau -> OneCycleLR and add LRFinder test.\n- v16: Second step trains on full dataset with 5 targets.\n- v17: Adds SWA for second step model, set `pct_start=0.1` and `patience=15` for step1. \n- v18: Iterative imputer set to 2 iterations.\n- v19: Iterative imputer set to 1 iteration & 50 features.\n- v20: Iterative imputer set to 1 iteration & 30 features. Hyperparams: `{'nn_width': 112, 'dropout': 0.3, 'momentum': 0.05, 'virtual_batch_size': 1024, 'weight_decay': 1e-05, 'pct_start': 0.5}` & `weight_decay=1e-5` - `pct_start=0.5`.\n- v21: Hyperparams: `{'nn_depth':3, 'nn_width': 112, 'dropout': 0.25, 'momentum': 0.09999999999999999, 'virtual_batch_size': 128, 'weight_decay': 0.0001, 'pct_start': 0.2}`\n- v23: Hyperparams: `{'nn_depth':3, 'nn_width': 96, 'dropout': 0.3, 'momentum': 0.1, 'virtual_batch_size': 256, 'weight_decay': 1e-05, 'pct_start': 0.3}`\n- v24: Hyperparams: `{'nn_depth': 3, 'nn_width': 96, 'dropout_input': 0.15, 'dropout_hidden': 0.3, 'dropout_output': 0.15, 'momentum':0.1, 'virtual_batch_size':256, 'weight_decay': 1e-05, 'pct_start': 0.4, 'max_lr': 0.06,}`. \n- v26: Visualizing the effect of SWA on validation set.\n- v27: Hyperparams: `{'nn_width': 96, 'dropout_input': 0.15, 'dropout_hidden': 0.4, 'dropout_output': 0.15, 'weight_decay': 1e-05, 'pct_start': 0.3, 'max_lr': 0.1}`.\n- v28: Hyperaprams: `nn_depth=3, nn_width=112, dropout_input=0.25, dropout_hidden=0.25, dropout_output=0.2, momentum=0.1, virtual_batch_size=256`, `weight_decay=1e-4`.\n- v29: SWA training with min_lr=1e-2, max_lr=1e-1, momentum=0.7 and num_epochs=15.\n- v30: Trained with sharp SWA on 2nd stage.\n- v31: Trained with lower regularization `dropout_input=0.2, dropout_hidden=0.25, dropout_output=0.15` and `momentum=0.8` on SWA."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import copy\nimport numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport pickle\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import OrderedDict\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.linear_model import BayesianRidge\nfrom category_encoders.one_hot import OneHotEncoder\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,TensorDataset,DataLoader\nfrom torch_lr_finder import LRFinder\nfrom torchsummary import summary\n\n# custom modules\nimport sys\nsys.path.append(\"../usr/lib/janestreet_torch_utils\")\nfrom janestreet_torch_utils import Monitor, train_step, valid_step\n\ndef set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    \nset_seed(2)\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def utility_score(date, weight, resp, action):\n    \"\"\"\n    Fast computation of utility score\n    \"\"\"\n    date = date.astype(int)\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) / np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 / count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return -u","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def cat_encoder(X):\n    \"\"\"\n    Fast one-hot encoding of feature_0\n    \"\"\"\n    X[\"feature_00\"] = 0\n    idx00 = X.query(\"feature_0 == -1\").index\n    X.loc[idx00,\"feature_00\"] = 1\n    \n    X[\"feature_01\"] = 0\n    idx01 = X.query(\"feature_0 == 1\").index\n    X.loc[idx01,\"feature_01\"] = 1\n    \n    return X.iloc[:,1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_metrics(monitor):\n    x = np.arange(len(monitor.train_loss))\n    \n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(21, 7))\n    \n    ax1 = axes[0]\n    ax2 = ax1.twinx()\n    ax1.plot(x, monitor.train_loss, 'go-', label=\"train_loss\")\n    ax2.plot(x, monitor.train_metric, 'ro-', label=\"train_metric\")\n    plt.legend(loc=\"best\")\n    ax1.set_xlabel('epochs')\n    ax1.set_ylabel('loss')\n    ax1.set_title(\"Training\")\n    plt.grid()\n    \n    ax1 = axes[1]\n    ax2 = ax1.twinx()\n    ax1.plot(x, monitor.valid_loss, 'go-', label=\"valid_loss\")\n    ax2.plot(x, monitor.valid_metric, 'ro-', label=\"valid_metric\")\n    plt.legend(loc=\"best\")\n    ax1.set_xlabel('epochs')\n    ax2.set_ylabel('metric')\n    ax1.set_title(\"Validation\")\n    plt.grid()\n    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"root = Path(\"../input/janestreet-preprocessing\")\n\ntrain = pd.read_parquet(root/\"train.parquet\")\nfeatures = pd.read_parquet(root/\"features.parquet\")\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query(\"date > 85\").query(\"weight > 0\").reset_index(drop=True)\n\ninput_features = [col for col in train.columns if \"feature\" in col]\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\nw_cols = [\"w\", \"w1\", \"w2\", \"w3\", \"w4\"]\n\nX_dset = train.loc[:,input_features].copy()\ny_dset = (train.loc[:,resp_cols] > 0).astype(int).copy()\nw_dset = train.loc[:, w_cols].copy()\ndwr_dset = train.loc[:, [\"date\",\"weight\",\"resp\"]].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\nwith open(\"../input/janestreet-imputer/imputer_f0m1.pickle\", \"rb\") as file:\n    imputer_f0m1 = pickle.load(file)\n    file.close()\n    \nwith open(\"../input/janestreet-imputer/imputer_f0p1.pickle\", \"rb\") as file:\n    imputer_f0p1 = pickle.load(file)\n    file.close()\n    \n    \nidx_f0m1 = X_dset.query(\"feature_0 == -1\").index\nX_dset.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_dset.loc[idx_f0m1, input_features[1:]])\n\nidx_f0p1 = X_dset.query(\"feature_0 ==  1\").index\nX_dset.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_dset.loc[idx_f0p1, input_features[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dset = cat_encoder(X_dset)\ninput_features = X_dset.columns.tolist()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### model definition"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BCELabelSmoothing(nn.Module):\n    def __init__(self, label_smoothing=0.0):\n        super(BCELabelSmoothing, self).__init__()\n        self.label_smoothing = label_smoothing\n        self.bce_loss = torch.nn.functional.binary_cross_entropy_with_logits\n        \n    def forward(self, prediction, target, weight=None):\n        target_smooth = target*(1.0 - self.label_smoothing) + 0.5*self.label_smoothing\n        if weight is None:\n            loss = self.bce_loss(prediction, target_smooth, reduction=\"mean\")\n        else:\n            loss = self.bce_loss(prediction, target_smooth, weight, reduction=\"sum\") / torch.sum(weight)\n        return loss\n\nbce_loss = BCELabelSmoothing(label_smoothing=1e-2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class GBN(nn.Module):\n    \"\"\"\n    Ghost Batch Normalization\n    https://arxiv.org/abs/1705.08741\n    \"\"\"\n\n    def __init__(self, input_dim, virtual_batch_size=128, momentum=0.01):\n        super(GBN, self).__init__()\n\n        self.input_dim = input_dim\n        self.virtual_batch_size = virtual_batch_size\n        self.bn = nn.BatchNorm1d(self.input_dim, momentum=momentum)\n\n    def forward(self, x):\n        chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n        res = [self.bn(x_) for x_ in chunks]\n\n        return torch.cat(res, dim=0)\n\n    \nclass NormalLinear(nn.Module):\n    \"\"\" \n    Linear layer with normalized weights\n    \"\"\"\n    def __init__(self, size_in, size_out, bias=True):\n        super(NormalLinear, self).__init__()\n        self.size_in, self.size_out = size_in, size_out\n        # weights vector\n        weights_v = torch.Tensor(size_out, size_in)\n        nn.init.kaiming_uniform_(weights_v, a=np.sqrt(5)) \n        self.weights_v = nn.Parameter(weights_v)\n        # weights magnitude\n        weights_m = torch.norm(weights_v, dim=1, keepdim=True)\n        self.weights_m = nn.Parameter(weights_m.clone().detach())\n        \n        if bias:\n            bias_v = torch.Tensor(size_out)    \n            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(weights_v)\n            bound = 1 / np.sqrt(fan_in)\n            nn.init.uniform_(bias_v, -bound, bound)\n            self.bias = nn.Parameter(bias_v)\n        else:\n            self.register_parameter('bias', None)\n            \n    def _compute_weights(self):\n        norm_per_output = torch.norm(self.weights_v, dim=1, keepdim=True)\n        return self.weights_m * torch.div(self.weights_v, norm_per_output)\n            \n    def forward(self, x):\n        weights = self._compute_weights()\n        return nn.functional.linear(x, weights, self.bias)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class SNN(nn.Module):\n    \"\"\"\n    Standard NN\n    \"\"\"\n\n    def __init__(self, input_dim, output_dim, nn_depth, nn_width, \n                 dropout_input=0.2, dropout_hidden=0.3, dropout_output=0.1, \n                 momentum=0.02, virtual_batch_size=128):\n        super().__init__()\n        \n        self.bn_in = GBN(input_dim, virtual_batch_size=virtual_batch_size, momentum=momentum)\n        self.dp_in = nn.Dropout(dropout_input)\n        self.ln_in = NormalLinear(input_dim, nn_width, bias=False)\n        \n        self.bnorms = nn.ModuleList(\n            [GBN(nn_width, virtual_batch_size=virtual_batch_size, momentum=momentum) \n             for i in range(nn_depth-1)])\n        self.dropouts = nn.ModuleList(\n            [nn.Dropout(dropout_hidden) \n             for i in range(nn_depth-1)])\n        self.linears = nn.ModuleList(\n            [NormalLinear(nn_width, nn_width, bias=False) \n             for i in range(nn_depth-1)])\n        \n        self.bn_out = GBN(nn_width, virtual_batch_size=virtual_batch_size, momentum=momentum)\n        self.dp_out = nn.Dropout(dropout_output)\n        self.ln_out = NormalLinear(nn_width, output_dim, bias=False)\n\n    def forward(self, x):\n        x = self.bn_in(x)\n        x = self.dp_in(x)\n        x = self.ln_in(x)\n        x = nn.functional.relu(x)\n\n        for bn_layer,dp_layer,ln_layer in zip(self.bnorms,self.dropouts,self.linears):\n            x = bn_layer(x)\n            x = dp_layer(x)\n            x = ln_layer(x)\n            x = nn.functional.relu(x)\n            \n        x = self.bn_out(x)\n        x = self.dp_out(x)\n        x = self.ln_out(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### model training: 1st step"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = train.query(\"date < 450\").index\nvalid_idx = train.query(\"date >= 450\").index\n\ntrain_dset = TensorDataset(torch.tensor(X_dset.loc[train_idx].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[train_idx].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[train_idx].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[train_idx].values, dtype=torch.float),\n                          )\n\nvalid_dset = TensorDataset(torch.tensor(X_dset.loc[valid_idx].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[valid_idx].values, dtype=torch.float),\n                          )\n\ndataset_sizes = {'train': len(train_dset), 'valid': len(valid_dset)}\ntrain_dataloader = DataLoader(train_dset, batch_size=2048, shuffle=True, num_workers=2)\nvalid_dataloader = DataLoader(valid_dset, batch_size=len(valid_dset), shuffle=False, num_workers=2)\n\nprint(\"Number of step per epoch:\", len(train_dset)//2048)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_model = SNN(input_dim=len(input_features), output_dim=len(resp_cols), nn_depth=3, nn_width=112, \n             dropout_input=0.20, dropout_hidden=0.30, dropout_output=0.20, \n             momentum=0.1, virtual_batch_size=256)\n_model = _model.to(device)\n\n_optimizer = torch.optim.SGD(_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\nlr_finder = LRFinder(_model, _optimizer, bce_loss, device=\"cuda\")\nlr_finder.range_test(train_dataloader, start_lr=1e-4, end_lr=1e1, num_iter=652*2, step_mode=\"exp\")\nlr_finder.plot(show_lr=1e-2)\nplt.show()\n\n_optimizer = torch.optim.SGD(_model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-4)\nlr_finder = LRFinder(_model, _optimizer, bce_loss, device=\"cuda\")\nlr_finder.range_test(train_dataloader, start_lr=1e-4, end_lr=1e1, num_iter=652*2, step_mode=\"exp\")\nlr_finder.plot(show_lr=1e-2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = SNN(input_dim=len(input_features), output_dim=len(resp_cols), nn_depth=3, nn_width=112, \n            dropout_input=0.20, dropout_hidden=0.30, dropout_output=0.20, \n            momentum=0.1, virtual_batch_size=256)\nmodel = model.to(device)\nsummary(model, input_size=(len(input_features),))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models_history = list()\n\noptimizer = torch.optim.SGD(model.parameters(), lr=2e-2, momentum=0.9, weight_decay=1e-4)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer, \n    max_lr=2e-2,\n    epochs=50,\n    pct_start=0.2,\n    anneal_strategy='cos', \n    cycle_momentum=True, \n    base_momentum=0.8, \n    max_momentum=0.9, \n    div_factor=1e1,\n    final_div_factor=1e0,\n    steps_per_epoch=len(train_dataloader),\n    verbose=False)\n\nmonitor = Monitor(\n    model=model,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    patience=10,\n    metric_fn=utility_score,\n    experiment_name=f'SNN',\n    num_epochs=50,\n    dataset_sizes=dataset_sizes,\n    early_stop_on_metric=False,\n    lower_is_better=True)\n\nfor epoch in monitor.iter_epochs:\n    train_step(model, train_dataloader, optimizer, monitor, bce_loss, scheduler=scheduler, clip_value=None)    \n    early_stop = valid_step(model, valid_dataloader, optimizer, monitor, bce_loss)\n    models_history.append(copy.deepcopy(model))\n    if early_stop: break","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics(monitor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saves all the models\nfor epoch,_model in enumerate(models_history):\n    torch.save(_model.state_dict(), f\"./snn-step1-epoch{epoch+1}.pt\")  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWA around optimal on validation data\nswa_model = torch.optim.swa_utils.AveragedModel(model)\nbest_epoch = len(models_history)-11\n\nfor _model in models_history[best_epoch:]:\n    swa_model.update_parameters(_model)\n    \ntorch.optim.swa_utils.update_bn(train_dataloader, swa_model, device=device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# puts state_dict of SWA in the format of original model\nstate_dict = OrderedDict()\nstate_dict_swa = swa_model.state_dict()\n\nfor key,params in state_dict_swa.items():\n    if key == \"n_averaged\": continue\n    key = key.replace(\"module.\",\"\")\n    state_dict[key] = params\n    \n# restore model to best averaged state\nmodel.load_state_dict(state_dict)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### model training: 2nd step"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_idx = train.query(\"date < 450\").index\nvalid_idx = train.query(\"date >= 450\").index\n\ntrain_dset = TensorDataset(torch.tensor(X_dset.loc[:].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[:].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[:].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[:].values, dtype=torch.float),\n                          )\n\nvalid_dset = TensorDataset(torch.tensor(X_dset.loc[valid_idx].values, dtype=torch.float), \n                           torch.tensor(y_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(w_dset.loc[valid_idx].values, dtype=torch.float),\n                           torch.tensor(dwr_dset.loc[valid_idx].values, dtype=torch.float),\n                          )\n\ndataset_sizes = {'train': len(train_dset), 'valid': len(valid_dset)}\ntrain_dataloader = DataLoader(train_dset, batch_size=2048, shuffle=True, num_workers=2)\nvalid_dataloader = DataLoader(valid_dset, batch_size=len(valid_dset), shuffle=False, num_workers=2)\n\nprint(\"Number of step per epoch:\", len(train_dset)//2048)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_optimizer = torch.optim.SGD(model.parameters(), lr=2e-3, momentum=0.8, weight_decay=1e-4)\nlr_finder = LRFinder(model, _optimizer, bce_loss, device=\"cuda\")\nlr_finder.range_test(train_dataloader, start_lr=1e-4, end_lr=1e1, num_iter=652*4, step_mode=\"exp\")\nlr_finder.plot(show_lr=1e-2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# restore to state of best epoch\nmodel.load_state_dict(state_dict)\noptimizer = torch.optim.SGD(model.parameters(), lr=2e-3, momentum=0.8, weight_decay=1e-4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scheduler = torch.optim.swa_utils.SWALR(optimizer, swa_lr=7e-3, anneal_strategy=\"cos\", anneal_epochs=5)\n\nmodels_history = list()\n\nmonitor = Monitor(\n    model=model,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    patience=35,\n    metric_fn=utility_score,\n    experiment_name=f'SNN',\n    num_epochs=35,\n    dataset_sizes=dataset_sizes,\n    early_stop_on_metric=False,\n    lower_is_better=True)\n\nfor epoch in monitor.iter_epochs:\n    train_step(model, train_dataloader, optimizer, monitor, bce_loss, scheduler=None, clip_value=None)    \n    valid_step(model, valid_dataloader, optimizer, monitor, bce_loss)\n    scheduler.step()\n    \n    models_history.append(copy.deepcopy(model))\n    \n    # save models along the path of swa\n    torch.save(model.state_dict(), f\"./snn-epoch{epoch+1}.pt\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"show_metrics(monitor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWA short training\nstart_epoch = 5\nend_epoch = 20\n\nswa_model_st = torch.optim.swa_utils.AveragedModel(model)\nswa_model_st = swa_model_st.to(device)\n\nfor _model in models_history[start_epoch:end_epoch]:\n    swa_model_st.update_parameters(_model)\n    \ntorch.optim.swa_utils.update_bn(train_dataloader, swa_model_st, device=device)\n\n# save final model for inference\ntorch.save(swa_model_st.state_dict(), \"./snn-swa-st.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# SWA long training\nstart_epoch = 5\nend_epoch = 35\n\nswa_model_lt = torch.optim.swa_utils.AveragedModel(model)\nswa_model_lt = swa_model_lt.to(device)\n\nfor _model in models_history[start_epoch:end_epoch]:\n    swa_model_lt.update_parameters(_model)\n    \ntorch.optim.swa_utils.update_bn(train_dataloader, swa_model_lt, device=device)\n\n# save final model for inference\ntorch.save(swa_model_lt.state_dict(), \"./snn-swa-lt.pt\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}