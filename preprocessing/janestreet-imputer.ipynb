{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nfrom pathlib import Path\nimport pickle\nimport time\nfrom tqdm import tqdm\nimport gc\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer,SimpleImputer\nfrom sklearn.linear_model import BayesianRidge, Ridge, HuberRegressor, RidgeCV\n\nTUNING = False\n\nnp.random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### preparing the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"root = Path(\"../input/janestreet-preprocessing\")\n\ntrain = pd.read_parquet(root/\"train.parquet\")\nfeatures = pd.read_parquet(root/\"features.parquet\")\n\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.query(\"date > 85\").query(\"weight > 0\").reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_features = [col for col in train.columns if \"feature\" in col]\nresp_cols = ['resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4']\n\nX_dset = train.loc[:,input_features].copy()\ny_dset = y_dset = (train.loc[:,resp_cols] > 0).astype(int).copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_dset.isna().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of rows with at least one nan value\n100 * X_dset.isna().any(axis=1).sum() / len(X_dset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# percentage of entries with nan values\n100*X_dset.isna().sum().sum() / (X_dset.shape[0]*X_dset.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature with nan values ordered from most to least\n(100*X_dset.isna().sum(axis=0)/len(X_dset)).sort_values(ascending=False).head(50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# feature without nan values\nprint(len(X_dset.isna().sum(axis=0)[X_dset.isna().sum(axis=0) == 0]))\nX_dset.isna().sum(axis=0)[X_dset.isna().sum(axis=0) == 0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### testing different imputers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def l1_distance(x, y):\n    return np.mean(np.abs(x-y))\n\ndef l2_distance(x, y):\n    return np.sqrt(np.mean((x-y)**2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TUNING:\n    nan_perc = X_dset.isna().sum(axis=0) / len(X_dset)\n    \n    X = X_dset.dropna().reset_index(drop=True)\n    X_nan = X.copy(deep=True)\n    \n    for feat,perc in nan_perc.iteritems():\n        idx = X[feat].sample(frac=perc).index.values\n        if len(idx)==0: continue\n        X_nan.loc[idx,feat] = np.nan\n\n    X_nan_train = X_nan.loc[:2*len(X_nan)//3,:].copy()\n    X_nan_valid = X_nan.loc[2*len(X_nan)//3+1:,:].copy()\n    nan_mask = np.isnan(X_nan_valid.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TUNING:\n    # imputer for feature_0 == -1 & feature_0 == 1\n    imputer_f0m1 = SimpleImputer(strategy=\"mean\")\n    imputer_f0p1 = SimpleImputer(strategy=\"mean\")\n\n    tic = time.time()\n    idx_f0m1 = X_nan_train.query(\"feature_0 == -1\").index\n    idx_f0p1 = X_nan_train.query(\"feature_0 == 1\").index\n    imputer_f0m1.fit(X_nan_train.loc[idx_f0m1, input_features[1:]])\n    imputer_f0p1.fit(X_nan_train.loc[idx_f0p1, input_features[1:]])\n    tac = time.time()\n    print(f\"Fit time: {(tac-tic)/60} min.\")\n\n    tic = time.time()\n    idx_f0m1 = X_nan_valid.query(\"feature_0 == -1\").index\n    idx_f0p1 = X_nan_valid.query(\"feature_0 == 1\").index\n    X_nan_valid_ = X_nan_valid.copy(deep=True)\n    X_nan_valid_.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_nan_valid_.loc[idx_f0m1, input_features[1:]])\n    X_nan_valid_.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_nan_valid_.loc[idx_f0p1, input_features[1:]])\n    tac = time.time()\n    print(f\"Predict time: {(tac-tic)/60} min.\")\n\n    l1 = l1_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n    l2 = l2_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n    print(f\"L1: {l1}  -  L2: {l2}\")\n\n    del imputer_f0m1,imputer_f0p1; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TUNING:\n    # imputer for feature_0 == -1 & feature_0 == 1\n    imputer_f0m1 = SimpleImputer(strategy=\"median\")\n    imputer_f0p1 = SimpleImputer(strategy=\"median\")\n\n    tic = time.time()\n    idx_f0m1 = X_nan_train.query(\"feature_0 == -1\").index\n    idx_f0p1 = X_nan_train.query(\"feature_0 == 1\").index\n    imputer_f0m1.fit(X_nan_train.loc[idx_f0m1, input_features[1:]])\n    imputer_f0p1.fit(X_nan_train.loc[idx_f0p1, input_features[1:]])\n    tac = time.time()\n    print(f\"Fit time: {(tac-tic)/60} min.\")\n\n    tic = time.time()\n    idx_f0m1 = X_nan_valid.query(\"feature_0 == -1\").index\n    idx_f0p1 = X_nan_valid.query(\"feature_0 == 1\").index\n    X_nan_valid_ = X_nan_valid.copy(deep=True)\n    X_nan_valid_.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_nan_valid_.loc[idx_f0m1, input_features[1:]])\n    X_nan_valid_.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_nan_valid_.loc[idx_f0p1, input_features[1:]])\n    tac = time.time()\n    print(f\"Predict time: {(tac-tic)/60} min.\")\n\n    l1 = l1_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n    l2 = l2_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n    print(f\"L1: {l1}  -  L2: {l2}\")\n\n    del imputer_f0m1,imputer_f0p1; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TUNING:\n    # imputer for feature_0 == -1 & feature_0 == 1\n    imputer_f0m1 = SimpleImputer(strategy=\"most_frequent\")\n    imputer_f0p1 = SimpleImputer(strategy=\"most_frequent\")\n\n    tic = time.time()\n    idx_f0m1 = X_nan_train.query(\"feature_0 == -1\").index\n    idx_f0p1 = X_nan_train.query(\"feature_0 == 1\").index\n    imputer_f0m1.fit(X_nan_train.loc[idx_f0m1, input_features[1:]])\n    imputer_f0p1.fit(X_nan_train.loc[idx_f0p1, input_features[1:]])\n    tac = time.time()\n    print(f\"Fit time: {(tac-tic)/60} min.\")\n\n    tic = time.time()\n    idx_f0m1 = X_nan_valid.query(\"feature_0 == -1\").index\n    idx_f0p1 = X_nan_valid.query(\"feature_0 == 1\").index\n    X_nan_valid_ = X_nan_valid.copy(deep=True)\n    X_nan_valid_.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_nan_valid_.loc[idx_f0m1, input_features[1:]])\n    X_nan_valid_.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_nan_valid_.loc[idx_f0p1, input_features[1:]])\n    tac = time.time()\n    print(f\"Predict time: {(tac-tic)/60} min.\")\n\n    l1 = l1_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n    l2 = l2_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n    print(f\"L1: {l1}  -  L2: {l2}\")\n\n    del imputer_f0m1,imputer_f0p1; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prev experiment showed that max_iter=1 gives best performance\n\nif TUNING:\n\n    for n_features in range(5,51,5):\n        print(f\" n_features: {n_features} \".center(60,\"-\"))\n\n        imputer_kwargs = dict(\n            estimator=BayesianRidge(normalize=True), \n            max_iter=1, \n            n_nearest_features=n_features,\n            initial_strategy=\"median\",\n            imputation_order=\"ascending\",\n            skip_complete=True,\n            verbose=0,\n            random_state=2,\n        )\n\n        # imputer for feature_0 == -1 & feature_0 == 1\n        imputer_f0m1 = IterativeImputer(**imputer_kwargs)\n        imputer_f0p1 = IterativeImputer(**imputer_kwargs)\n\n        tic = time.time()\n        idx_f0m1 = X_nan_train.query(\"feature_0 == -1\").index\n        idx_f0p1 = X_nan_train.query(\"feature_0 == 1\").index\n        imputer_f0m1.fit(X_nan_train.loc[idx_f0m1, input_features[1:]])\n        imputer_f0p1.fit(X_nan_train.loc[idx_f0p1, input_features[1:]])\n        tac = time.time()\n        print(f\"Fit time: {(tac-tic)/60} min.\")\n\n        tic = time.time()\n        idx_f0m1 = X_nan_valid.query(\"feature_0 == -1\").index\n        idx_f0p1 = X_nan_valid.query(\"feature_0 == 1\").index\n        X_nan_valid_ = X_nan_valid.copy(deep=True)\n        X_nan_valid_.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_nan_valid_.loc[idx_f0m1, input_features[1:]])\n        X_nan_valid_.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_nan_valid_.loc[idx_f0p1, input_features[1:]])\n        tac = time.time()\n        print(f\"Predict time: {(tac-tic)/60} min.\")\n\n        l1 = l1_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n        l2 = l2_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\n        print(f\"L1: {l1}  -  L2: {l2}\")\n\n        del imputer_f0m1,imputer_f0p1; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# without normalization \n\nimputer_kwargs = dict(\n    estimator=BayesianRidge(normalize=False), \n    max_iter=1, \n    n_nearest_features=30,\n    initial_strategy=\"median\",\n    imputation_order=\"ascending\",\n    skip_complete=True,\n    verbose=0,\n    random_state=2,\n)\n\n# imputer for feature_0 == -1 & feature_0 == 1\nimputer_f0m1 = IterativeImputer(**imputer_kwargs)\nimputer_f0p1 = IterativeImputer(**imputer_kwargs)\n\ntic = time.time()\nidx_f0m1 = X_nan_train.query(\"feature_0 == -1\").index\nidx_f0p1 = X_nan_train.query(\"feature_0 == 1\").index\nimputer_f0m1.fit(X_nan_train.loc[idx_f0m1, input_features[1:]])\nimputer_f0p1.fit(X_nan_train.loc[idx_f0p1, input_features[1:]])\ntac = time.time()\nprint(f\"Fit time: {(tac-tic)/60} min.\")\n\ntic = time.time()\nidx_f0m1 = X_nan_valid.query(\"feature_0 == -1\").index\nidx_f0p1 = X_nan_valid.query(\"feature_0 == 1\").index\nX_nan_valid_ = X_nan_valid.copy(deep=True)\nX_nan_valid_.loc[idx_f0m1, input_features[1:]] = imputer_f0m1.transform(X_nan_valid_.loc[idx_f0m1, input_features[1:]])\nX_nan_valid_.loc[idx_f0p1, input_features[1:]] = imputer_f0p1.transform(X_nan_valid_.loc[idx_f0p1, input_features[1:]])\ntac = time.time()\nprint(f\"Predict time: {(tac-tic)/60} min.\")\n\nl1 = l1_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\nl2 = l2_distance(X_nan_valid_.values[nan_mask], X.loc[X_nan_valid.index].values[nan_mask])\nprint(f\"L1: {l1}  -  L2: {l2}\")\n\ndel imputer_f0m1,imputer_f0p1; gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***\n### fitting the imputer"},{"metadata":{"trusted":true},"cell_type":"code","source":"imputer_kwargs = dict(\n    estimator=BayesianRidge(normalize=True), \n    max_iter=1, \n    n_nearest_features=30,\n    initial_strategy=\"median\",\n    imputation_order=\"ascending\",\n    skip_complete=True,\n    verbose=1,\n    random_state=2,\n)\n\n# imputer for feature_0 == -1 & feature_0 == 1\nimputer_f0m1 = IterativeImputer(**imputer_kwargs)\nimputer_f0p1 = IterativeImputer(**imputer_kwargs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimputer_f0m1.fit(X_dset.query(\"feature_0 == -1\").loc[:, input_features[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nimputer_f0p1.fit(X_dset.query(\"feature_0 == 1\").loc[:, input_features[1:]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# testing inference time\nX_sample = X_dset[X_dset.isna().any(axis=1)].sample(5000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_times = list()\nimputer_f0m1.verbose = False\nimputer_f0p1.verbose = False\n\nfor i,row in tqdm(X_sample.iterrows()):\n    tic = time.time()\n    if row.feature_0 < 0:\n        _ = imputer_f0m1.transform(row.values[1:].reshape(1,-1))\n    else:\n        _ = imputer_f0p1.transform(row.values[1:].reshape(1,-1))\n    tac = time.time()\n    all_times.append(tac-tic)\n\nprint(\"Inference-time per sample:\", np.mean(np.asarray(all_times)*1000), \"[ms]\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(imputer_f0m1.imputation_sequence_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(imputer_f0p1.imputation_sequence_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"imputer_f0m1.pickle\", \"wb\") as file:\n    pickle.dump(imputer_f0m1, file, protocol=pickle.HIGHEST_PROTOCOL)\n    file.close()\n    \nwith open(\"imputer_f0p1.pickle\", \"wb\") as file:\n    pickle.dump(imputer_f0p1, file, protocol=pickle.HIGHEST_PROTOCOL)\n    file.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}